# GSoC 2025: SITL AI Reinforcement Learning Concept Script

![GSoC banner](images/GSoC-Banner.png)



> **A Lua script that lets ArduPilot learn to tune itself, live, within SITL.**

**Mentors:** Nate Mailhot & Sanket Sharma&nbsp;&nbsp;|&nbsp;&nbsp;**Timeframe:** May ‚Üí Sep 2025&nbsp;&nbsp;|&nbsp;&nbsp;**Project:** [GSoC](https://summerofcode.withgoogle.com/programs/2025/projects/w7EYZSIz)

## 1. Introduction

Hello ArduPilot community! I'm Bruno Andreoni Sarmento, a 4th year student at the Polytechnic School of the University of S√£o Paulo (USP), pursuing a bachelor‚Äôs degree in Electrical Engineering. For the past two years I‚Äôve been a member of Skyrats (USP‚Äôs autonomous-drone team), where I‚Äôve worked with many open-source tools, ArduPilot being our core flight-control framework.

I've been inspired by other team members that took part in GSoC project, and it was a dream come true being selected to this awesome project!

## 2. Problem Statement

Today, UAV parameter tuning still relies on manual adjustments, often requiring trial-and-error sweeps and test flights. This approach is:

- Time-consuming and inefficient;
- Poorly reproducible;
- Not scalable for adaptive or autonomous systems.

Meanwhile, **Reinforcement Learning (RL)** offers a principled method for adaptive control, but the **ArduPilot SITL environment lacks built-in support for episodic interactions**, which are essential for reliable training and evaluation.

## 3. Project Proposal
> This project builds a lightweight Lua-based scripting layer on top of the existing SITL engine to enable episodic RL interactions.

![RL Illustrated](images/rl.jpg)

The project introduces:

- **Episodic Replay**: A structured way to test and evaluate UAV behavior across multiple runs, starting from known or randomized states.
- **State Reset Mechanism**: Using `sim:set_pose()` in Lua scripts to programmatically reset UAV position, velocity, and attitude between episodes.
- **Metric Logging**: Automated telemetry collection (e.g., tracking error, convergence time) to evaluate each episode‚Äôs performance.
- **Online RL Loop**: A lightweight **online algorithm** (e.g., SARSA or Q-learning) will run in real-time within SITL to update parameters during simulation.

This project builds the necessary infrastructure for **online learning directly in SITL**, unlocking new workflows for intelligent parameter tuning and experimental reinforcement learning within the ArduPilot ecosystem.

## 4. Solution Diagram
![Solution diagram](images/diagram.png)

This diagram shows how **online reinforcement learning** is embedded into **ArduPilot SITL** using **Lua scripting**.

### 4.1 Block‚Äëby‚ÄëBlock Breakdown¬†üîç

| Block                 | Role                                                                                  | Key ArduPilot / Lua APIs                                       |
| --------------------- | ------------------------------------------------------------------------------------- | -------------------------------------------------------------- |
| **Episode¬†Manager**   | Detects episode termination (e.g. time‚Äëout, crash, success) and fires a reset.        | `time:millis()`, custom state checks                           |
| **Teleport¬†/¬†Reset**  | Instantaneously re‚Äëpositions vehicle & zeros attitude/vel using **`sim:set_pose()`**. | `sim:set_pose(instance, loc, orient, vel_bf, gyro)`            |
| **Metrics¬†Collector** | Computes reward signals (tracking error, overshoot, energy) every tick.               | `ahrs:get_pitch()`, `vehicle:get_target_alt()`, custom math    |
| **Online¬†RL**         | Updates Q‚Äëtable / value function in‚Äëplace at \~10‚ÄØHz; selects next action.            | Pure Lua tables for Q‚Äëtable; math lib                          |
| **Policy¬†Executor**   | Applies action by tweaking params or sending direct control:                          | `param:set("ATC_RAT_PIT_P",¬†val)`, `vehicle:set_target_roll()` |
| **EKF3**              | Supplies fused state to Lua via binding helpers (`ahrs`, `ins`, etc.).                | Firmware internal                                              |
| **Sensor¬†Simulation** | Generates IMU/GPS from physics backend (Gazebo or built‚Äëin).                          | SITL¬†C++                                                       |

## 5. How the Loop Runs
1. **Reset** ‚Üí Script arms vehicle, immediately calls `sim:set_pose()` to starting state.
2. **Episode** ‚Üí RL chooses an action (e.g., ¬±5‚ÄØ% on `ATC_RAT_RLL_P`).
3. **Flight** ‚Üí Controller responds; Metrics¬†Collector accumulates reward.
4. **Terminate** ‚Üí After `N`¬†steps or goal reached, Episode¬†Manager logs result and triggers new reset.
5. **Learn** ‚Üí RL updates its policy online before next episode begins.

Every tick is scheduled via `return loop, 100` (100‚ÄØms).
### 5.1 Code sketch
A simplified sketch of how the episodic learning loop works in Lua:

```lua
-- init
local episode   = EpisodeManager.new{max_steps=200}
local policy    = QLearning.new{alpha=0.1, gamma=0.9, eps=0.2}
local collector = Metrics.new()

function loop()
  if episode:done() then
    sim:set_pose(0, episode.start_loc, episode.start_ori)
    policy:learn(collector:get())
    episode:reset()
    collector:clear()
  else
    local s = episode:get_state()
    local a = policy:select(s)
    PolicyExecutor.apply(a)
    collector:record(s, a)
  end
  return loop, 100 -- ms
end

return loop, 100
```

*(Full implementation lives in https://github.com/b-andreoni/GSoC/blob/main/scripts)*

---


## 6. Timeline
This GSoC project is structured into four main phases:

| Phase                    | Timeframe    | Key Deliverables                                           |
|--------------------------|--------------|------------------------------------------------------------|
| **1. Reset Mechanism**   | June 2025    | - Integrate `sim:set_pose()` into SITL Lua API<br>- Enable repeatable episode resets |
| **2. Episodic RL Loop**  | July 2025    | - Implement reward & metrics collection<br>- Build basic online RL loop (Q-learning/SARSA) |
| **3. Policy Refinement** | August 2025  | - Add exploration strategies (e.g. Œµ-decay)<br>- Validate on varied flight scenarios |
| **4. Docs & Releases**   | Early Sept   | - Publish user guide and examples<br>- Open PRs/issues for community review |

## 7. Call for feedback
I'm looking for feedback from the ArduPilot community on:

- Reset strategies using sim:set_pose()

- Good reward signals for tuning (especially velocity/PID)

- Use cases where episodic RL could help your workflow

