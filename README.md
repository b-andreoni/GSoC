# Google Summer of Code 2025: SITL AI Reinforcement Learning Concept Script

![GSoC banner](docs/images/GSoC-Banner.png)

> **A Lua script that lets ArduPilot learn to tune itself, live, within SITL.**

**Mentors:** Nate Mailhot & Sanket SharmaÂ Â |Â Â **Timeframe:** Mayâ€¯â†’â€¯SepÂ 2025Â Â |Â Â **Project:** [GSoC](https://summerofcode.withgoogle.com/programs/2025/projects/w7EYZSIz)

## 1. Introduction ğŸ“£

Hello ArduPilot community! I'm Bruno Andreoni Sarmento, a 4thâ€‘year student at the Polytechnic School of the University of SÃ£oÂ Paulo (USP), pursuing a bachelorâ€™s degree in Electrical Engineering. For the past two years Iâ€™ve been a member of Skyrats (USPâ€™s autonomousâ€‘drone team), where Iâ€™ve worked with many openâ€‘source tools, ArduPilot being our core flightâ€‘control framework.

I've been inspired by other team members that took part in GSoC projects, and it was a dream come true being selected for this awesome project!

## 2. Problem Statement â—

Today, UAV parameter tuning still relies on manual adjustments, often requiring trialâ€‘andâ€‘error sweeps and test flights. This approach is:

* Timeâ€‘consuming and inefficient;
* Poorly reproducible;
* Not scalable for adaptive or autonomous systems.

Meanwhile, **Reinforcement Learning (RL)** offers a principled method for adaptive control, but the **ArduPilot SITL environment lacks builtâ€‘in support for episodic interactions**, which are essential for reliable training and evaluation.

## 3. Project Proposal ğŸš€

> This project builds a lightweight Luaâ€‘based scripting layer on top of the existing SITL engine to enable episodic RL interactions.

![RL Illustrated](docs/images/rl.jpg)

The project introduces:

* **Episodic Replay**Â â€“ a structured way to test and evaluate UAV behavior across multiple runs, starting from known or randomized states;
* **State Reset Mechanism**Â â€“ using `sim:set_pose()` in Lua scripts to programmatically reset UAV position, velocity, and attitude between episodes;
* **Metric Logging**Â â€“ automated telemetry collection (e.g., tracking error, convergence time) to evaluate each episodeâ€™s performance;
* **OfflineÂ RL Phase (baseline)**Â â€“ log rich episodes and train an initial Qâ€‘table/valueâ€‘function *offline* before any inâ€‘place updates;
* **OnlineÂ RL Phase (final goal)**Â â€“ once the baseline is set, the same agent keeps learning *live*, updating parameters in real time during simulation.

This project builds the necessary infrastructure for **online learning directly in SITL**, unlocking new workflows for intelligent parameter tuning and experimental reinforcement learning within the ArduPilot ecosystem.

## 4. Solution Diagram ğŸ–¼ï¸

![Solution diagram](docs/images/diagram.png)

### 4.1 Blockâ€‘byâ€‘Block BreakdownÂ ğŸ”

| Block                              | Role                                                                                                                                                                                                                                                    | Key ArduPilotÂ / Lua APIs                                       |
| ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------- |
| **EpisodeÂ Manager**                | Detects episode termination (e.g. timeâ€‘out, crash, success) and fires a reset.                                                                                                                                                                          | `time:millis()`, custom state checks                           |
| **TeleportÂ /Â Reset**               | Instantaneously reâ€‘positions vehicle & zeros attitude/vel using **`sim:set_pose()`**.                                                                                                                                                                   | `sim:set_pose(instance, loc, orient, vel_bf, gyro)`            |
| **MetricsÂ Collector**              | Computes reward signals (tracking error, overshoot, energy) every tick.                                                                                                                                                                                 | `ahrs:get_pitch()`, `vehicle:get_target_alt()`, custom math    |
| **Reinforcement Learning**                      | Updates Qâ€‘table / value function inâ€‘place at â‰ˆâ€¯10â€¯Hz; selects next action. During the **OfflineÂ RL Phase** it runs in *logâ€‘only* mode to gather stateâ€“actionâ€“reward traces for baseline training, switching to live updates in the **OnlineÂ RL Phase**. | Pure Lua tables for Qâ€‘table; math lib                          |
| **PolicyÂ Executor**                | Applies action by tweaking params or sending direct control:                                                                                                                                                                                            | `param:set("ATC_RAT_PIT_P",Â val)`, `vehicle:set_target_roll()` |
| **Direct State Feed (EKFÂ OptÂ 10)** | Supplies fused state to Lua via binding helpers (`ahrs`, `ins`, etc.).                                                                                                                                                                                  | Firmware internal                                              |
| **SensorÂ Simulation**              | Generates IMU/GPS from physics backend (Gazebo or builtâ€‘in).                                                                                                                                                                                            | SITLÂ C++                                                       |

## 5. How the Loop Runs ğŸ”„

1. **Reset** â†’ Script arms vehicle, immediately calls `sim:set_pose()` to starting state.
2. **Episode** â†’ RL chooses an action (e.g., Â±5â€¯% on `ATC_RAT_RLL_P`).
3. **Flight** â†’ Controller responds; MetricsÂ Collector accumulates reward.
4. **Terminate** â†’ After `N`Â steps or goal reached, EpisodeÂ Manager logs result and triggers new reset.
5. **Learn** â†’ RL updates its policy online before next episode begins.

Every tick is scheduled via `return loop, 100` (100â€¯ms).

### 5.1 Code sketch ğŸ’»

```lua
-- init
local episode   = EpisodeManager.new{max_steps=200}
local policy    = QLearning.new{alpha=0.1, gamma=0.9, eps=0.2}
local collector = Metrics.new()

function loop()
  if episode:done() then
    sim:set_pose(0, episode.start_loc, episode.start_ori)
    policy:learn(collector:get())
    episode:reset()
    collector:clear()
  else
    local s = episode:get_state()
    local a = policy:select(s)
    PolicyExecutor.apply(a)
    collector:record(s, a)
  end
  return loop, 100 -- ms
end

return loop, 100
```

*(Full implementation lives in [https://github.com/b-andreoni/GSoC/blob/main/scripts](https://github.com/b-andreoni/GSoC/blob/main/scripts))*

---

## 6. Timeline ğŸ“…

This GSoC project is structured into five main phases:

| Phase                              | Timeframe                   | Key Deliverables                                                                                                          |
| ---------------------------------- | --------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| **1. Reset Mechanism**             | JuneÂ 2025                   | â€¢ Integrate `sim:set_pose()` into SITL Lua API<br>â€¢ Enable repeatable episode resets                                      |
| **2. OfflineÂ RL Phase (baseline)** | lateâ€¯Juneâ€¯â€“â€¯JulyÂ 2025       | â€¢ Collect episodic logs with a fixed policy<br>â€¢ Train baseline value/Qâ€‘function offline<br>â€¢ Report baseline performance |
| **3. OnlineÂ RL Loop**              | JulyÂ 2025                   | â€¢ Implement reward & metrics collection<br>â€¢ Activate inâ€‘place learning (SARSA/Qâ€‘learning)                                |
| **4. Policy Refinement**           | AugustÂ 2025                 | â€¢ Add exploration strategies (e.g., Îµâ€‘decay)<br>â€¢ Validate on varied flight scenarios                                     |
| **5. Docs & Releases**             | EarlyÂ SepÂ 2025              | â€¢ Publish user guide and examples<br>â€¢ Open PRs/issues for community review                                               |

## 7. Call for Feedback ğŸ’¬

Iâ€™d love to hear your fieldâ€‘tested wisdom, war stories, and wishâ€‘list items:

* **Reset mechanics** â€“ any EKF glitches, timing tricks, or failsafes youâ€™ve hit when using `sim:set_pose()`?
* **Practical benefits** â€“ where could an inâ€‘sim offline+online RL loop spare you manual PID sweeps or repetitive test flights?

Drop a comment on the PR, open an issue, or ping me on Discord (@bruno\_as). Your input will directly shape the next commits.

Thanks in advance!

## 8. GitHub ğŸ”—

[https://github.com/b-andreoni/GSoC/](https://github.com/b-andreoni/GSoC/)

### Related PRs & Code Snippets

* [https://github.com/ArduPilot/ardupilot/pull/29616](https://github.com/ArduPilot/ardupilot/pull/29616)
* [https://github.com/ArduPilot/ardupilot/blob/master/libraries/AP\_Scripting/examples/sim\_arming\_pos.lua](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AP_Scripting/examples/sim_arming_pos.lua)
* [https://github.com/ArduPilot/ardupilot/pull/29498/commits/51364c7f43af09c5f42d3e95315dc9c642dc093c](https://github.com/ArduPilot/ardupilot/pull/29498/commits/51364c7f43af09c5f42d3e95315dc9c642dc093c)



# Mid-Term Project Update  
> **Reinforcement Learning is already possible**

Hello again, ArduPilot community! Below is a concise overview of the milestones reached, and obstacles overcome during the first half of GSoC 2025.

## 1. What has been accomplished so far

### 1.1 `set_pose()` + mission integration  
The initial task was to extend the example script [`sim_arming_pos.lua`](https://github.com/ArduPilot/ardupilot/blob/c90c3ae6a67356c59f0a07a498f682d94b874459/libraries/AP_Scripting/examples/sim_arming_pos.lua#L64) so that a teleport could be executed after arming, followed immediately by any mission of interest.

* [**AUTO mode**](https://github.com/b-andreoni/GSoC/blob/main/scripts/mission/set_pose%2Bmission.lua) - After arming, the vehicle is teleported anywhere in the world, takes off, and then switches to AUTO to fly a pre-saved mission.  
* [**GUIDED mode**](https://github.com/b-andreoni/GSoC/blob/main/scripts/mission/set_pose%2Btarget.lua) - With AUTO working, waypoints are now injected dynamically via GUIDED, providing full path control from Lua.

A limitation emerged: subsequent teleports reused the *initial* NED offset, which quickly became impractical. This motivated a deeper change to `sim:set_pose()`.

### 1.2 Local modifications to `sim:set_pose()`  
Thanks to @tridge, we already had a working `set_pose`! 
However, episodic resets were still impossible because only the NED offset was updated; updating global position more than once was impossible. The solution required editing **`ardupilot/libraries/SITL/SIM_Aircraft.cpp`**:

```cpp
// original
aircraft.position = aircraft.home.get_distance_NED_double(loc);

// replacement
aircraft.home   = loc;
aircraft.origin = loc;
aircraft.position = Vector3d(0, 0, 0);
```

With *home*, *origin*, and *position* realigned, each call to `sim:set_pose()` now places the vehicle at the exact intended global location, enabling clean episode restarts.
**The complete patch is already pushed to my public fork:**
https://github.com/b-andreoni/ardupilot
Feel free to clone it instead of patching the file manually.

### 1.3 Parameters logging  
To evaluate learning performance, a [CSV logger](https://github.com/b-andreoni/GSoC/blob/main/scripts/log/log.lua) was created that records

```
elapsed_ms, fsm_state, flight_mode, is_armed,
lat_deg, lon_deg, alt_m,
vel_n, vel_e, vel_d,
roll, pitch, yaw, yaw_bhv,
gyro_x, gyro_y, gyro_z,
volt_v, curr_a
```
A flag is inserted whenever a reset occurs, allowing precise segmentation of episodes.

### 1.4 Customisable initial conditions at [episodic resets](https://github.com/b-andreoni/GSoC/blob/main/scripts/reset/episodic_reset.lua)
Reset hooks allow deterministic tweaks - Â”e.g. yaw can be rotated by 45Â° at every episode start.
![Yaw increment|442x84, 75%](upload://nf21jHs38hso3D7ptAog17R5iuI.png)


### 1.5 First Reinforcement-Learning prototype 
![|545x500, 75%](upload://e3xp7XGUwEoT8r1chnLEem3It8c.png)
A lightweight **online Q-learning loop** was implemented to optimise best path to visit five waypoints energy-minimizing (inspired by the [IMAV 2024](https://2024.imavs.org/competition) first task). Workflow:

1. Teleport to home via `sim:set_pose()`.  
2. Take off to 10 m AGL.  
3. Select the next waypoint Îµ-greedily (Îµ = 0.2, decaying).  
4. Log voltage ÃƒÂ— current every 50 ms; **negative energy** is the reward.  
5. Apply a Q-update at each waypoint (`Î± = 0.1`, `Î³ = 0.9`).  
6. End-of-episode: compare total energy to best-so-far; repeat.  

Convergence to the optimal path occurs after Ã¢Â‰Âˆ 200 episodes.

---

## 2. Current implementation

To reproduce the results:

1. Patch **`SIM_Aircraft.cpp`** as below and rebuild SITL, or clone the forked repo:
![SIM_Aircraft modifications|690x232](upload://yiUpSiHn0QMYRHrhqsgS1N7fjRd.png)
```
git clone git@github.com:b-andreoni/ardupilot.git
cd ardupilot
./waf configure --board sitl
./waf copter
```
2. Clone the scripts repo at https://github.com/b-andreoni/GSoC/tree/main. 
`git clone git@github.com:b-andreoni/GSoC.git`
4. Enable Lua scripts: `param set SCR_ENABLE 1`.  
5. Copy `scripts/reinforcement_learning/rl_wp.lua` into the working `scripts` folder and run  

```bash
sim_vehicle.py -v ArduCopter --map
```

---

## 3. Results so far

* **Teleport reliability**: A stress test of consecutive [resets](https://github.com/b-andreoni/GSoC/blob/main/scripts/reset/episodic_reset.lua) shows no EKF divergence.  

https://youtu.be/MgVrWxLkT7Y

* **Energy-optimal path**: Q-learning finds the best path between waypoints.

https://youtu.be/zplOf3A-gOk  


---

## 4. Next steps and final-submission goals

* Publish additional **toy examples** (e.g., PID tuning for precision landing and waypoint centralization, plane flare optimization).  
* Write a **wiki-style guide** highlighting the reset API, logging template, and common pitfalls.  
* Polish reinforcement-learning.
* Gather community feedback.
* Stretch goal: **Gazebo / Isaac Sim bridge** - Investigate adapter layers for their reset functions, enabling cross-sim RL experiments.

Feedback and suggestions, mainly for desired tunable parameters, are very welcome!
