# Google Summer of Code 2025: SITL AI Reinforcement Learning Concept Script

![GSoC banner](docs/images/GSoC-Banner.png)

> **A Lua script that lets ArduPilot learn to tune itself, live, within SITL.**

**Mentors:** Nate Mailhot & Sanket SharmaÂ Â |Â Â **Timeframe:** Mayâ€¯â†’â€¯SepÂ 2025Â Â |Â Â **Project:** [GSoC](https://summerofcode.withgoogle.com/programs/2025/projects/w7EYZSIz)

## 1. Introduction ğŸ“£

Hello ArduPilot community! I'm Bruno Andreoni Sarmento, a 4thâ€‘year student at the Polytechnic School of the University of SÃ£oÂ Paulo (USP), pursuing a bachelorâ€™s degree in Electrical Engineering. For the past two years Iâ€™ve been a member of Skyrats (USPâ€™s autonomousâ€‘drone team), where Iâ€™ve worked with many openâ€‘source tools, ArduPilot being our core flightâ€‘control framework.

I've been inspired by other team members that took part in GSoC projects, and it was a dream come true being selected for this awesome project!

## 2. Problem Statement â—

Today, UAV parameter tuning still relies on manual adjustments, often requiring trialâ€‘andâ€‘error sweeps and test flights. This approach is:

* Timeâ€‘consuming and inefficient;
* Poorly reproducible;
* Not scalable for adaptive or autonomous systems.

Meanwhile, **Reinforcement Learning (RL)** offers a principled method for adaptive control, but the **ArduPilot SITL environment lacks builtâ€‘in support for episodic interactions**, which are essential for reliable training and evaluation.

## 3. Project Proposal ğŸš€

> This project builds a lightweight Luaâ€‘based scripting layer on top of the existing SITL engine to enable episodic RL interactions.

![RL Illustrated](docs/images/rl.jpg)

The project introduces:

* **Episodic Replay**Â â€“ a structured way to test and evaluate UAV behavior across multiple runs, starting from known or randomized states;
* **State Reset Mechanism**Â â€“ using `sim:set_pose()` in Lua scripts to programmatically reset UAV position, velocity, and attitude between episodes;
* **Metric Logging**Â â€“ automated telemetry collection (e.g., tracking error, convergence time) to evaluate each episodeâ€™s performance;
* **OfflineÂ RL Phase (baseline)**Â â€“ log rich episodes and train an initial Qâ€‘table/valueâ€‘function *offline* before any inâ€‘place updates;
* **OnlineÂ RL Phase (final goal)**Â â€“ once the baseline is set, the same agent keeps learning *live*, updating parameters in real time during simulation.

This project builds the necessary infrastructure for **online learning directly in SITL**, unlocking new workflows for intelligent parameter tuning and experimental reinforcement learning within the ArduPilot ecosystem.

## 4. Solution Diagram ğŸ–¼ï¸

![Solution diagram](docs/images/diagram.png)

### 4.1 Blockâ€‘byâ€‘Block BreakdownÂ ğŸ”

| Block                              | Role                                                                                                                                                                                                                                                    | Key ArduPilotÂ / Lua APIs                                       |
| ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------- |
| **EpisodeÂ Manager**                | Detects episode termination (e.g. timeâ€‘out, crash, success) and fires a reset.                                                                                                                                                                          | `time:millis()`, custom state checks                           |
| **TeleportÂ /Â Reset**               | Instantaneously reâ€‘positions vehicle & zeros attitude/vel using **`sim:set_pose()`**.                                                                                                                                                                   | `sim:set_pose(instance, loc, orient, vel_bf, gyro)`            |
| **MetricsÂ Collector**              | Computes reward signals (tracking error, overshoot, energy) every tick.                                                                                                                                                                                 | `ahrs:get_pitch()`, `vehicle:get_target_alt()`, custom math    |
| **Reinforcement Learning**                      | Updates Qâ€‘table / value function inâ€‘place at â‰ˆâ€¯10â€¯Hz; selects next action. During the **OfflineÂ RL Phase** it runs in *logâ€‘only* mode to gather stateâ€“actionâ€“reward traces for baseline training, switching to live updates in the **OnlineÂ RL Phase**. | Pure Lua tables for Qâ€‘table; math lib                          |
| **PolicyÂ Executor**                | Applies action by tweaking params or sending direct control:                                                                                                                                                                                            | `param:set("ATC_RAT_PIT_P",Â val)`, `vehicle:set_target_roll()` |
| **Direct State Feed (EKFÂ OptÂ 10)** | Supplies fused state to Lua via binding helpers (`ahrs`, `ins`, etc.).                                                                                                                                                                                  | Firmware internal                                              |
| **SensorÂ Simulation**              | Generates IMU/GPS from physics backend (Gazebo or builtâ€‘in).                                                                                                                                                                                            | SITLÂ C++                                                       |

## 5. How the Loop Runs ğŸ”„

1. **Reset** â†’ Script arms vehicle, immediately calls `sim:set_pose()` to starting state.
2. **Episode** â†’ RL chooses an action (e.g., Â±5â€¯% on `ATC_RAT_RLL_P`).
3. **Flight** â†’ Controller responds; MetricsÂ Collector accumulates reward.
4. **Terminate** â†’ After `N`Â steps or goal reached, EpisodeÂ Manager logs result and triggers new reset.
5. **Learn** â†’ RL updates its policy online before next episode begins.

Every tick is scheduled via `return loop, 100` (100â€¯ms).

### 5.1 Code sketch ğŸ’»

```lua
-- init
local episode   = EpisodeManager.new{max_steps=200}
local policy    = QLearning.new{alpha=0.1, gamma=0.9, eps=0.2}
local collector = Metrics.new()

function loop()
  if episode:done() then
    sim:set_pose(0, episode.start_loc, episode.start_ori)
    policy:learn(collector:get())
    episode:reset()
    collector:clear()
  else
    local s = episode:get_state()
    local a = policy:select(s)
    PolicyExecutor.apply(a)
    collector:record(s, a)
  end
  return loop, 100 -- ms
end

return loop, 100
```

*(Full implementation lives in [https://github.com/b-andreoni/GSoC/blob/main/scripts](https://github.com/b-andreoni/GSoC/blob/main/scripts))*

---

## 6. Timeline ğŸ“…

This GSoC project is structured into five main phases:

| Phase                              | Timeframe                   | Key Deliverables                                                                                                          |
| ---------------------------------- | --------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| **1. Reset Mechanism**             | JuneÂ 2025                   | â€¢ Integrate `sim:set_pose()` into SITL Lua API<br>â€¢ Enable repeatable episode resets                                      |
| **2. OfflineÂ RL Phase (baseline)** | lateâ€¯Juneâ€¯â€“â€¯earlyâ€¯JulyÂ 2025 | â€¢ Collect episodic logs with a fixed policy<br>â€¢ Train baseline value/Qâ€‘function offline<br>â€¢ Report baseline performance |
| **3. OnlineÂ RL Loop**              | JulyÂ 2025                   | â€¢ Implement reward & metrics collection<br>â€¢ Activate inâ€‘place learning (SARSA/Qâ€‘learning)                                |
| **4. Policy Refinement**           | AugustÂ 2025                 | â€¢ Add exploration strategies (e.g., Îµâ€‘decay)<br>â€¢ Validate on varied flight scenarios                                     |
| **5. Docs & Releases**             | EarlyÂ SepÂ 2025              | â€¢ Publish user guide and examples<br>â€¢ Open PRs/issues for community review                                               |

## 7. Call for Feedback ğŸ’¬

Iâ€™d love to hear your fieldâ€‘tested wisdom, war stories, and wishâ€‘list items:

* **Reset mechanics** â€“ any EKF glitches, timing tricks, or failsafes youâ€™ve hit when using `sim:set_pose()`?
* **Practical benefits** â€“ where could an inâ€‘sim offline+online RL loop spare you manual PID sweeps or repetitive test flights?

Drop a comment on the PR, open an issue, or ping me on Discord (@bruno\_as). Your input will directly shape the next commits.

Thanks in advance!

## 8. GitHub ğŸ”—

[https://github.com/b-andreoni/GSoC/](https://github.com/b-andreoni/GSoC/)

### Related PRs & Code Snippets

* [https://github.com/ArduPilot/ardupilot/pull/29616](https://github.com/ArduPilot/ardupilot/pull/29616)
* [https://github.com/ArduPilot/ardupilot/blob/master/libraries/AP\_Scripting/examples/sim\_arming\_pos.lua](https://github.com/ArduPilot/ardupilot/blob/master/libraries/AP_Scripting/examples/sim_arming_pos.lua)
* [https://github.com/ArduPilot/ardupilot/pull/29498/commits/51364c7f43af09c5f42d3e95315dc9c642dc093c](https://github.com/ArduPilot/ardupilot/pull/29498/commits/51364c7f43af09c5f42d3e95315dc9c642dc093c)
